{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 2편: 완전연결 신경망으로 FashionMNIST 분류하기\n",
    "\n",
    "이 노트북에서는 다음을 학습합니다:\n",
    "- FashionMNIST 데이터셋의 구조와 시각화\n",
    "- DataLoader를 사용한 미니배치 학습\n",
    "- 1계층 완전연결(FC) 신경망 구현\n",
    "- ReLU 활성화 함수와 Dropout을 적용한 다계층 FC 신경망 구현\n",
    "- 테스트 정확도 평가 및 혼동 행렬(Confusion Matrix) 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. FashionMNIST 데이터셋\n",
    "\n",
    "FashionMNIST는 Zalando에서 공개한 패션 아이템 이미지 데이터셋입니다.\n",
    "MNIST 숫자 데이터셋과 동일한 포맷(28x28 그레이스케일)이지만, 의류 아이템을 분류하는 더 어려운 과제입니다.\n",
    "\n",
    "| 항목 | 내용 |\n",
    "|---|---|\n",
    "| 이미지 크기 | 28 x 28 그레이스케일 |\n",
    "| 학습 데이터 | 60,000장 |\n",
    "| 테스트 데이터 | 10,000장 |\n",
    "| 클래스 | 10가지 (T-shirt, Trouser, Pullover 등) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# FashionMNIST 다운로드\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# 클래스 이름 매핑\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "print(f\"학습 데이터: {len(training_data)}장\")\n",
    "print(f\"테스트 데이터: {len(test_data)}장\")\n",
    "print(f\"클래스 수: {len(class_names)}\")\n",
    "print(f\"이미지 shape: {training_data[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 시각화 (2x5 그리드)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = training_data[i]\n",
    "    ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "    ax.set_title(class_names[label], fontsize=10)\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"FashionMNIST Samples\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. DataLoader로 배치 학습 준비\n",
    "\n",
    "PyTorch의 데이터 파이프라인은 두 핵심 클래스로 구성됩니다:\n",
    "\n",
    "- **Dataset**: 개별 샘플(이미지, 레이블)에 인덱스로 접근하는 인터페이스\n",
    "- **DataLoader**: Dataset을 감싸서 미니배치 단위로 데이터를 공급하는 반복자\n",
    "\n",
    "**batch_size=64**로 설정합니다. 배치 크기는 학습 안정성과 메모리 사용량 사이의 균형을 결정합니다.\n",
    "작은 배치는 노이즈가 많지만 일반화 성능이 좋고, 큰 배치는 안정적이지만 메모리를 많이 사용합니다.\n",
    "\n",
    "**Flatten**: 28x28 이미지를 784차원 벡터로 펼쳐야 완전연결 계층에 입력할 수 있습니다.\n",
    "`x.view(-1, 784)` 또는 `x.flatten(1)`로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"전체 학습 데이터: {len(training_data)}장\")\n",
    "print(f\"배치 크기: {batch_size}\")\n",
    "print(f\"학습 배치 수: {len(train_loader)} (= ceil({len(training_data)} / {batch_size}))\")\n",
    "print(f\"테스트 배치 수: {len(test_loader)}\")\n",
    "\n",
    "# 첫 번째 배치 확인\n",
    "sample_images, sample_labels = next(iter(train_loader))\n",
    "print(f\"\\n배치 이미지 shape: {sample_images.shape}\")\n",
    "print(f\"  -> {sample_images.shape[0]}장 x {sample_images.shape[2]}x{sample_images.shape[3]} 이미지 x {sample_images.shape[1]} 채널\")\n",
    "print(f\"배치 레이블 shape: {sample_labels.shape}\")\n",
    "\n",
    "# Flatten 변환 시연\n",
    "flattened = sample_images.view(-1, 28 * 28)\n",
    "print(f\"\\nFlatten 전: {sample_images.shape}\")\n",
    "print(f\"Flatten 후: {flattened.shape}  (28 x 28 = 784차원 벡터)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. 1계층 완전연결 신경망\n",
    "\n",
    "가장 단순한 구조로, 입력을 바로 출력 클래스에 매핑합니다.\n",
    "\n",
    "```\n",
    "입력 (28x28) -> Flatten (784) -> Linear(784, 10) -> 출력 (10 클래스)\n",
    "```\n",
    "\n",
    "**파라미터 수 계산**:\n",
    "- 가중치: 784 x 10 = 7,840개\n",
    "- 편향(bias): 10개\n",
    "- **합계: 7,850개**\n",
    "\n",
    "단순하지만, 선형 변환만으로는 복잡한 패턴을 학습하기 어렵습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "\n",
    "\n",
    "class SingleLayerNet(nn.Module):\n",
    "    \"\"\"1계층 완전연결 신경망: 784 -> 10\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_out = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc_out(x)\n",
    "\n",
    "\n",
    "fc_single = SingleLayerNet().to(device)\n",
    "print(fc_single)\n",
    "\n",
    "single_params = sum(p.numel() for p in fc_single.parameters())\n",
    "print(f\"\\n총 파라미터 수: {single_params:,}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1계층 모델 학습\n",
    "optimizer_single = torch.optim.Adam(fc_single.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epochs = 15\n",
    "single_costs = []\n",
    "single_times = []\n",
    "\n",
    "fc_single.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 784).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer_single.zero_grad()\n",
    "        outputs = fc_single(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_single.step()\n",
    "\n",
    "        running_loss += loss.item() / num_batches\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    single_costs.append(running_loss)\n",
    "    single_times.append(elapsed)\n",
    "    print(f\"Epoch {epoch + 1:02d}/{num_epochs} | loss = {running_loss:.6f} | time = {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\n평균 에포크 시간: {np.mean(single_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. 다계층 완전연결 신경망\n",
    "\n",
    "1계층 모델의 한계를 극복하기 위해 **은닉층(hidden layer)**을 추가합니다.\n",
    "핵심 차이점은 계층 사이에 **ReLU 활성화 함수**와 **Dropout**을 적용하는 것입니다.\n",
    "\n",
    "```\n",
    "입력(784) -> Linear(784, 512) -> ReLU -> Dropout(0.3)\n",
    "          -> Linear(512, 256) -> ReLU -> Dropout(0.3)\n",
    "          -> Linear(256, 10)  -> 출력(10 클래스)\n",
    "```\n",
    "\n",
    "**ReLU (Rectified Linear Unit)**: `max(0, x)` - 비선형성을 도입하여 복잡한 패턴을 학습할 수 있게 합니다.\n",
    "활성화 함수 없이 선형 계층만 쌓으면 결국 하나의 선형 변환과 동일합니다.\n",
    "\n",
    "**Dropout(0.3)**: 학습 시 뉴런의 30%를 무작위로 비활성화하여 과적합을 방지합니다.\n",
    "\n",
    "**파라미터 수 계산**:\n",
    "- 1층: 784 x 512 + 512 = 401,920\n",
    "- 2층: 512 x 256 + 256 = 131,328\n",
    "- 3층: 256 x 10 + 10 = 2,570\n",
    "- **합계: 535,818개** (1계층 대비 약 68배)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNet(nn.Module):\n",
    "    \"\"\"다계층 완전연결 신경망: 784 -> 512 -> 256 -> 10 (ReLU + Dropout)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "fc_multi = MultiLayerNet().to(device)\n",
    "print(fc_multi)\n",
    "\n",
    "multi_params = sum(p.numel() for p in fc_multi.parameters())\n",
    "print(f\"\\n총 파라미터 수: {multi_params:,}개\")\n",
    "print(f\"1계층 대비 {multi_params / single_params:.0f}배\")\n",
    "\n",
    "# 계층별 파라미터 수 상세\n",
    "print(\"\\n[계층별 파라미터 수]\")\n",
    "for name, param in fc_multi.named_parameters():\n",
    "    print(f\"  {name:20s} -> {param.numel():>10,}개  {list(param.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다계층 모델 학습\n",
    "optimizer_multi = torch.optim.Adam(fc_multi.parameters(), lr=0.001)\n",
    "\n",
    "multi_costs = []\n",
    "multi_times = []\n",
    "\n",
    "fc_multi.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 784).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer_multi.zero_grad()\n",
    "        outputs = fc_multi(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_multi.step()\n",
    "\n",
    "        running_loss += loss.item() / num_batches\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    multi_costs.append(running_loss)\n",
    "    multi_times.append(elapsed)\n",
    "    print(f\"Epoch {epoch + 1:02d}/{num_epochs} | loss = {running_loss:.6f} | time = {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\n평균 에포크 시간: {np.mean(multi_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. 테스트 정확도 평가\n",
    "\n",
    "학습이 끝난 모델을 **테스트 데이터**로 평가합니다.\n",
    "테스트 데이터는 학습 과정에서 한 번도 사용하지 않은 데이터이므로, 모델의 일반화 성능을 측정할 수 있습니다.\n",
    "\n",
    "평가 시에는 반드시 `model.eval()`과 `torch.no_grad()`를 사용합니다:\n",
    "- `model.eval()`: Dropout 비활성화, BatchNorm을 추론 모드로 전환\n",
    "- `torch.no_grad()`: 그래디언트 계산을 중단하여 메모리 절약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    \"\"\"모델을 평가하여 정확도, 예측값, 실제값을 반환한다.\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.view(-1, 784).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "# 두 모델 평가\n",
    "single_acc, single_preds, single_labels = evaluate(fc_single, test_loader)\n",
    "multi_acc, multi_preds, multi_labels = evaluate(fc_multi, test_loader)\n",
    "\n",
    "print(f\"1계층 모델 테스트 정확도: {single_acc:.4f} ({single_acc * 100:.2f}%)\")\n",
    "print(f\"다계층 모델 테스트 정확도: {multi_acc:.4f} ({multi_acc * 100:.2f}%)\")\n",
    "\n",
    "# 클래스별 정확도 계산 (다계층 모델)\n",
    "per_class_correct = np.zeros(10)\n",
    "per_class_total = np.zeros(10)\n",
    "for pred, label in zip(multi_preds, multi_labels):\n",
    "    per_class_total[label] += 1\n",
    "    if pred == label:\n",
    "        per_class_correct[label] += 1\n",
    "\n",
    "per_class_acc = per_class_correct / per_class_total\n",
    "\n",
    "# 클래스별 정확도 수평 막대 차트\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors = plt.cm.RdYlGn(per_class_acc)  # 정확도에 따라 색상 변화\n",
    "bars = ax.barh(range(10), per_class_acc * 100, color=colors, edgecolor=\"gray\", linewidth=0.5)\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_yticklabels(class_names)\n",
    "ax.set_xlabel(\"Accuracy (%)\")\n",
    "ax.set_title(f\"Per-Class Accuracy (MultiLayerNet, Overall: {multi_acc * 100:.1f}%)\")\n",
    "ax.set_xlim(0, 100)\n",
    "ax.axvline(x=multi_acc * 100, color=\"red\", linestyle=\"--\", alpha=0.7, label=f\"Average: {multi_acc * 100:.1f}%\")\n",
    "\n",
    "# 막대 끝에 수치 표시\n",
    "for i, (bar, acc) in enumerate(zip(bars, per_class_acc)):\n",
    "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{acc * 100:.1f}%\", va=\"center\", fontsize=9)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 혼동 행렬 (Confusion Matrix) 계산 및 시각화\n",
    "confusion_mat = np.zeros((10, 10), dtype=int)\n",
    "for pred, label in zip(multi_preds, multi_labels):\n",
    "    confusion_mat[label][pred] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(confusion_mat, cmap=\"Blues\")\n",
    "\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha=\"right\", fontsize=9)\n",
    "ax.set_yticklabels(class_names, fontsize=9)\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "ax.set_title(\"Confusion Matrix (MultiLayerNet)\")\n",
    "\n",
    "# 셀에 수치 표시\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        value = confusion_mat[i][j]\n",
    "        color = \"white\" if value > confusion_mat.max() * 0.5 else \"black\"\n",
    "        ax.text(j, i, str(value), ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. 1계층 vs 다계층 비교\n",
    "\n",
    "두 모델의 학습 과정과 성능을 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "# Loss 비교\n",
    "axes[0].plot(epochs_range, single_costs, \"o-\", markersize=4,\n",
    "             label=f\"SingleLayerNet ({single_params:,} params)\")\n",
    "axes[0].plot(epochs_range, multi_costs, \"s-\", markersize=4,\n",
    "             label=f\"MultiLayerNet ({multi_params:,} params)\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 에포크별 학습 시간 비교\n",
    "axes[1].plot(epochs_range, single_times, \"o-\", markersize=4, label=\"SingleLayerNet\")\n",
    "axes[1].plot(epochs_range, multi_times, \"s-\", markersize=4, label=\"MultiLayerNet\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Time (seconds)\")\n",
    "axes[1].set_title(\"Training Time per Epoch\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 요약 테이블\n",
    "print(f\"{'='*65}\")\n",
    "print(f\"{'항목':20s} {'SingleLayerNet':>18s} {'MultiLayerNet':>18s}\")\n",
    "print(f\"{'-'*65}\")\n",
    "print(f\"{'파라미터 수':20s} {single_params:>18,} {multi_params:>18,}\")\n",
    "print(f\"{'최종 학습 loss':20s} {single_costs[-1]:>18.6f} {multi_costs[-1]:>18.6f}\")\n",
    "print(f\"{'테스트 정확도':20s} {single_acc * 100:>17.2f}% {multi_acc * 100:>17.2f}%\")\n",
    "print(f\"{'평균 에포크 시간':20s} {np.mean(single_times):>17.2f}s {np.mean(multi_times):>17.2f}s\")\n",
    "print(f\"{'='*65}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 완전연결 신경망의 한계\n",
    "\n",
    "현재는 28x28의 아주 작은 그레이스케일 이미지를 다루고 있지만,\n",
    "실제 환경에서는 훨씬 큰 이미지를 처리해야 합니다.\n",
    "\n",
    "**4K 이미지 (3840 x 2160 x 3 = 24,883,200차원)를 FC 레이어에 입력하면?**\n",
    "- 첫 번째 은닉층이 512개 뉴런만 가져도: 24,883,200 x 512 = **약 127억 개의 파라미터**\n",
    "- float32 기준 약 **47GB의 메모리**가 필요\n",
    "- 학습이 사실상 불가능\n",
    "\n",
    "**근본적인 문제**: 완전연결 신경망은 이미지를 1차원 벡터로 펼치기 때문에\n",
    "**공간적 구조(인접 픽셀 간의 관계)**를 전혀 활용하지 못합니다.\n",
    "\n",
    "예를 들어, 이미지에서 \"가장자리\"나 \"질감\" 같은 지역적 패턴은\n",
    "인접한 픽셀들의 관계에서 나오는데, FC 레이어는 모든 픽셀을 독립적으로 취급합니다.\n",
    "\n",
    "**다음 노트북에서 CNN(합성곱 신경망)**으로 이 문제를 해결합니다.\n",
    "CNN은 작은 필터가 이미지 위를 슬라이딩하며 지역 패턴을 추출하므로,\n",
    "파라미터 수를 크게 줄이면서도 공간 정보를 보존합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": "application/x-python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}