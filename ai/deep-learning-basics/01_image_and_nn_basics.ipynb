{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1편: 이미지의 디지털 표현과 신경망 첫걸음\n",
    "\n",
    "이 노트북에서는 다음을 학습합니다:\n",
    "- 이미지가 컴퓨터에서 어떻게 숫자 배열로 표현되는지\n",
    "- RGB 채널의 의미와 분리 방법\n",
    "- PyTorch 텐서와 Linear 레이어 기초\n",
    "- 학습의 핵심 사이클: Forward → Loss → Backward → Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이미지는 숫자다\n",
    "\n",
    "컴퓨터가 보는 이미지는 픽셀 값으로 이루어진 3차원 배열입니다.\n",
    "- **height**: 세로 픽셀 수\n",
    "- **width**: 가로 픽셀 수\n",
    "- **channels**: 색상 채널 수 (RGB = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import copy\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# 샘플 이미지 다운로드\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/47/PNG_transparency_demonstration_1.png/280px-PNG_transparency_demonstration_1.png\"\n",
    "img_path = \"sample_image.png\"\n",
    "if not os.path.exists(img_path):\n",
    "    urllib.request.urlretrieve(img_url, img_path)\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img = np.array(img)\n",
    "print(f\"이미지 shape: {np_img.shape}\")\n",
    "print(f\"  → 높이={np_img.shape[0]}px, 너비={np_img.shape[1]}px, 채널={np_img.shape[2]}(RGB)\")\n",
    "print(f\"\\n픽셀 값 범위: {np_img.min()} ~ {np_img.max()}\")\n",
    "print(f\"\\n좌측 상단 5x5 영역의 Red 채널 값:\\n{np_img[:5, :5, 0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 크롭(Crop)\n",
    "\n",
    "numpy 슬라이싱으로 이미지의 특정 영역을 잘라낼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 중앙 영역을 크롭\n",
    "h, w = np_img.shape[:2]\n",
    "crop_h, crop_w = 100, 150\n",
    "start_h, start_w = h // 2 - crop_h // 2, w // 2 - crop_w // 2\n",
    "cropped = np_img[start_h:start_h+crop_h, start_w:start_w+crop_w]\n",
    "\n",
    "print(f\"크롭 전: {np_img.shape} → 크롭 후: {cropped.shape}\")\n",
    "Image.fromarray(cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB 채널 분리\n",
    "\n",
    "이미지는 Red, Green, Blue 3개 채널로 구성됩니다. 각 채널만 남기면 해당 색상 성분을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# 원본\n",
    "axes[0].imshow(cropped)\n",
    "axes[0].set_title(\"Original\")\n",
    "\n",
    "# 각 채널 분리\n",
    "channel_names = [\"Red\", \"Green\", \"Blue\"]\n",
    "for i, name in enumerate(channel_names):\n",
    "    channel_img = np.zeros_like(cropped)\n",
    "    channel_img[:, :, i] = cropped[:, :, i]\n",
    "    axes[i + 1].imshow(channel_img)\n",
    "    axes[i + 1].set_title(f\"{name} Channel\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그레이스케일 변환\n",
    "grey = np.mean(cropped, axis=2).astype(np.uint8)\n",
    "print(f\"그레이스케일 shape: {grey.shape} (채널 차원이 사라짐)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(cropped)\n",
    "axes[0].set_title(\"Color\")\n",
    "axes[1].imshow(grey, cmap=\"gray\")\n",
    "axes[1].set_title(\"Grayscale\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch 텐서 소개\n",
    "\n",
    "PyTorch는 GPU 가속을 지원하는 텐서 연산 라이브러리입니다.\n",
    "numpy 배열과 유사하지만, **자동 미분(autograd)** 기능이 핵심입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# numpy 배열 → PyTorch 텐서\n",
    "np_arr = np.array([1.0, 2.0, 3.0])\n",
    "tensor = torch.from_numpy(np_arr)\n",
    "print(f\"numpy:  {np_arr}, dtype={np_arr.dtype}\")\n",
    "print(f\"tensor: {tensor}, dtype={tensor.dtype}\")\n",
    "\n",
    "# 디바이스 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\n사용 디바이스: {device}\")\n",
    "\n",
    "# 텐서 연산\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(f\"\\n덧셈: {a + b}\")\n",
    "print(f\"행렬곱: {torch.dot(a, b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 신경망의 가장 작은 단위: Linear 레이어\n",
    "\n",
    "Linear 레이어는 다음 연산을 수행합니다:\n",
    "\n",
    "$$y = xW^T + b$$\n",
    "\n",
    "- **x**: 입력 (input_features 차원)\n",
    "- **W**: 가중치 행렬 (output_features × input_features)\n",
    "- **b**: 편향 벡터 (output_features 차원)\n",
    "- **y**: 출력 (output_features 차원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 4차원 입력 → 2차원 출력 Linear 레이어\n",
    "linear = nn.Linear(4, 2, bias=True).to(device)\n",
    "\n",
    "print(f\"가중치(W) shape: {linear.weight.shape}\")\n",
    "print(f\"편향(b) shape:   {linear.bias.shape}\")\n",
    "print(f\"\\n초기 가중치:\\n{linear.weight.data}\")\n",
    "print(f\"초기 편향: {linear.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 핵심 사이클\n",
    "\n",
    "딥러닝 학습은 4단계의 반복입니다:\n",
    "\n",
    "1. **Forward (순전파)**: 입력을 모델에 통과시켜 예측값을 계산\n",
    "2. **Loss (손실 계산)**: 예측값과 정답의 차이를 측정\n",
    "3. **Backward (역전파)**: 손실에 대한 각 가중치의 기울기를 계산\n",
    "4. **Step (가중치 업데이트)**: 기울기 방향으로 가중치를 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 분류 문제 설정\n",
    "inputs = np.array([2.0, 4.0, 5.0, 6.0])\n",
    "target_class = 0  # 클래스 0으로 분류되어야 하는 입력\n",
    "\n",
    "X = torch.Tensor(inputs).view(1, -1).to(device)   # (1, 4) 형태\n",
    "Y = torch.tensor([target_class]).to(device)        # 정답 레이블\n",
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)\n",
    "\n",
    "print(f\"입력 X shape: {X.shape}\")\n",
    "print(f\"정답 Y: {Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 학습 1 사이클 상세 관찰 ===\n",
    "\n",
    "# 1단계: Forward (순전파)\n",
    "optimizer.zero_grad()  # 기울기 초기화 (누적 방지)\n",
    "prediction = linear(X)\n",
    "print(f\"[Forward] 예측값: {prediction.data}\")\n",
    "print(f\"  → 클래스 0 점수: {prediction.data[0][0]:.4f}, 클래스 1 점수: {prediction.data[0][1]:.4f}\")\n",
    "\n",
    "# 2단계: Loss (손실 계산)\n",
    "loss = criterion(prediction, Y)\n",
    "print(f\"\\n[Loss] 손실값: {loss.item():.4f}\")\n",
    "\n",
    "# 3단계: Backward (역전파) - 기울기 계산\n",
    "print(f\"\\n[Backward 전] weight.grad: {linear.weight.grad}\")\n",
    "loss.backward()\n",
    "print(f\"[Backward 후] weight.grad:\\n{linear.weight.grad}\")\n",
    "\n",
    "# 4단계: Step (가중치 업데이트)\n",
    "weight_before = linear.weight.data.clone()\n",
    "optimizer.step()\n",
    "weight_after = linear.weight.data.clone()\n",
    "\n",
    "print(f\"\\n[Step] 가중치 변화:\")\n",
    "print(f\"  Before: {weight_before[0][:2].tolist()}...\")\n",
    "print(f\"  After:  {weight_after[0][:2].tolist()}...\")\n",
    "print(f\"  차이:   {(weight_after - weight_before)[0][:2].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 반복 학습으로 손실 줄이기\n",
    "\n",
    "위의 4단계를 여러 번 반복하면, 가중치가 점차 최적값에 수렴하며 손실이 줄어듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 에포크 학습\n",
    "losses = []\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "    prediction = linear(X)\n",
    "    loss = criterion(prediction, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:02d} | loss = {loss.item():.6f}\")\n",
    "\n",
    "# 학습 과정 시각화\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(1, 21), losses, marker=\"o\", markersize=4)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n최종 가중치:\\n{linear.weight.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "| 개념 | 설명 |\n",
    "|---|---|\n",
    "| 이미지 | 숫자로 이루어진 3차원 배열 (H × W × C) |\n",
    "| 텐서 | PyTorch의 기본 데이터 구조 (GPU 연산 + 자동 미분 지원) |\n",
    "| Linear 레이어 | y = xW^T + b 연산을 수행하는 신경망의 기본 단위 |\n",
    "| 학습 사이클 | Forward → Loss → Backward → Step 의 반복 |\n",
    "\n",
    "**다음 노트북에서는** 실제 이미지 데이터셋(FashionMNIST)을 사용하여 완전연결 신경망을 학습해봅니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
