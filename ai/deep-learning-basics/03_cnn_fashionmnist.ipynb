{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 3편: CNN으로 이미지 분류하기\n",
    "\n",
    "이 노트북에서는 다음을 학습합니다:\n",
    "- 합성곱(Convolution) 연산의 원리와 FC 레이어와의 차이\n",
    "- CNN 핵심 구성 요소: Conv2d, BatchNorm2d, ReLU, MaxPool2d, Dropout\n",
    "- 3-블록 CNN 아키텍처로 FashionMNIST 분류\n",
    "- 텐서 shape가 각 레이어를 통과하며 어떻게 변하는지 추적\n",
    "- 특징 맵(Feature Map) 시각화로 CNN 내부 동작 이해\n",
    "- FC NN vs CNN 성능 비교 (정확도, 학습 시간, 파라미터 수)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. 합성곱(Convolution)이란?\n",
    "\n",
    "FC(Fully Connected) 레이어는 이미지의 모든 픽셀을 1차원으로 펼쳐서 처리합니다.  \n",
    "이 과정에서 **인접 픽셀 간의 공간적 관계(지역성)**가 완전히 사라집니다.\n",
    "\n",
    "합성곱은 작은 **필터(커널)**를 이미지 위에서 슬라이딩하며 특징을 추출합니다:\n",
    "\n",
    "| 특성 | FC 레이어 | 합성곱 레이어 |\n",
    "|---|---|---|\n",
    "| 연결 방식 | 모든 입력 ↔ 모든 출력 | **지역적 연결** (커널 크기만큼) |\n",
    "| 파라미터 | 입력 × 출력 개수 | **커널 크기 × 채널 수** (공유) |\n",
    "| 공간 정보 | 1D로 펼쳐서 소실 | **2D 구조 유지** |\n",
    "| 파라미터 수 | 이미지 크기에 비례 | 이미지 크기와 **무관** |\n",
    "\n",
    "핵심 원리:\n",
    "- **지역적 연결(Local Connectivity)**: 전체가 아닌 작은 영역(예: 5×5, 3×3)만 연결\n",
    "- **파라미터 공유(Parameter Sharing)**: 동일한 필터를 이미지 전체에 재사용\n",
    "- **결과**: 파라미터 수가 이미지 크기와 무관하게 대폭 감소!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# FashionMNIST 데이터 준비\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "print(f\"학습 데이터: {len(training_data):,}장\")\n",
    "print(f\"테스트 데이터: {len(test_data):,}장\")\n",
    "print(f\"배치 크기: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. CNN 핵심 구성 요소\n",
    "\n",
    "이번 CNN에서 사용할 주요 레이어를 정리합니다:\n",
    "\n",
    "| 레이어 | 역할 | 이번 모델에서의 사용 |\n",
    "|---|---|---|\n",
    "| `Conv2d` | 합성곱 연산으로 특징 추출 (커널이 이미지 위를 슬라이딩) | 5×5, 3×3 커널 |\n",
    "| `BatchNorm2d` | 각 채널별로 정규화하여 학습 안정화 및 가속 | 모든 Conv 뒤에 적용 |\n",
    "| `ReLU` | 비선형성 추가 (음수 → 0, 양수 → 그대로) | 활성화 함수 |\n",
    "| `MaxPool2d` | 공간 크기를 절반으로 축소 (2×2 영역에서 최대값 추출) | Block 1, 2에서 사용 |\n",
    "| `Dropout` | 학습 중 일부 뉴런을 무작위 비활성화 (과적합 방지) | Classifier에서 사용 |\n",
    "| `Linear` | 최종 분류를 위한 완전연결 레이어 | 3136→128→10 |\n",
    "\n",
    "`BatchNorm2d`는 이전 노트북에서는 사용하지 않았던 새로운 요소입니다.  \n",
    "각 미니배치에 대해 채널별 평균/분산을 정규화하여 **학습 속도 향상**과 **안정적 수렴**에 기여합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape 추적: 텐서가 각 레이어를 통과하며 어떻게 변하는지 확인\n",
    "sample = torch.randn(1, 1, 28, 28)\n",
    "print(f\"입력:               {sample.shape}  ← (배치, 채널, 높이, 너비)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Block 1: Conv(1→16, 5×5, padding=2) + MaxPool(2)\n",
    "print(\"\\n[Block 1]\")\n",
    "x = nn.Conv2d(1, 16, kernel_size=5, padding=2)(sample)\n",
    "print(f\"Conv2d(1→16, 5×5):  {x.shape}  ← padding=2로 크기 유지\")\n",
    "x = nn.BatchNorm2d(16)(x)\n",
    "print(f\"BatchNorm2d(16):    {x.shape}  ← 채널별 정규화 (shape 불변)\")\n",
    "x = F.relu(x)\n",
    "print(f\"ReLU:               {x.shape}  ← 활성화 (shape 불변)\")\n",
    "x = F.max_pool2d(x, 2)\n",
    "print(f\"MaxPool2d(2):       {x.shape}  ← 28/2 = 14\")\n",
    "\n",
    "# Block 2: Conv(16→32, 3×3, padding=1) + MaxPool(2)\n",
    "print(\"\\n[Block 2]\")\n",
    "x = nn.Conv2d(16, 32, kernel_size=3, padding=1)(x)\n",
    "print(f\"Conv2d(16→32, 3×3): {x.shape}  ← padding=1로 크기 유지\")\n",
    "x = nn.BatchNorm2d(32)(x)\n",
    "print(f\"BatchNorm2d(32):    {x.shape}\")\n",
    "x = F.relu(x)\n",
    "print(f\"ReLU:               {x.shape}\")\n",
    "x = F.max_pool2d(x, 2)\n",
    "print(f\"MaxPool2d(2):       {x.shape}  ← 14/2 = 7\")\n",
    "\n",
    "# Block 3: Conv(32→64, 3×3, padding=1) (풀링 없음)\n",
    "print(\"\\n[Block 3]\")\n",
    "x = nn.Conv2d(32, 64, kernel_size=3, padding=1)(x)\n",
    "print(f\"Conv2d(32→64, 3×3): {x.shape}  ← padding=1로 크기 유지\")\n",
    "x = nn.BatchNorm2d(64)(x)\n",
    "print(f\"BatchNorm2d(64):    {x.shape}\")\n",
    "x = F.relu(x)\n",
    "print(f\"ReLU:               {x.shape}\")\n",
    "\n",
    "# Flatten → Classifier\n",
    "print(\"\\n[Classifier]\")\n",
    "flat = x.view(1, -1)\n",
    "print(f\"Flatten:            {flat.shape}  ← 64 × 7 × 7 = 3,136\")\n",
    "out = nn.Linear(64 * 7 * 7, 128)(flat)\n",
    "print(f\"Linear(3136→128):   {out.shape}\")\n",
    "out = nn.Linear(128, 10)(out)\n",
    "print(f\"Linear(128→10):     {out.shape}  ← 10개 클래스 점수\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. CNN 모델 구현\n",
    "\n",
    "3개의 합성곱 블록과 분류기(Classifier)로 구성된 CNN을 설계합니다:\n",
    "\n",
    "```\n",
    "입력 (1, 28, 28)\n",
    "  ┌─────────────────────────────────────────────┐\n",
    "  │ Block 1: Conv(1→16, 5×5) + BN + ReLU + Pool │ → (16, 14, 14)\n",
    "  ├─────────────────────────────────────────────┤\n",
    "  │ Block 2: Conv(16→32, 3×3) + BN + ReLU + Pool│ → (32, 7, 7)\n",
    "  ├─────────────────────────────────────────────┤\n",
    "  │ Block 3: Conv(32→64, 3×3) + BN + ReLU       │ → (64, 7, 7)\n",
    "  └─────────────────────────────────────────────┘\n",
    "  Flatten → (3136)\n",
    "  ┌─────────────────────────────────────────────┐\n",
    "  │ Classifier: Dropout(0.3) + Linear(3136→128) │\n",
    "  │           + ReLU + Dropout(0.5)              │\n",
    "  │           + Linear(128→10)                   │ → (10)\n",
    "  └─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "설계 포인트:\n",
    "- **3개 Conv 블록**: 점진적으로 채널 수 증가 (1→16→32→64)\n",
    "- **BatchNorm2d**: 모든 Conv 뒤에 적용하여 학습 안정화\n",
    "- **첫 번째 블록에 5×5 커널**: 넓은 수용 영역(receptive field)으로 저수준 특징 포착\n",
    "- **padding 사용**: 합성곱 후 공간 크기가 줄어들지 않도록 제어\n",
    "- **LogSoftmax 없이 CrossEntropyLoss 직접 사용**: PyTorch 권장 패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Block 1: Conv(1→16, 5×5) + BN + ReLU + MaxPool(2)\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),  # (16, 28, 28)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (16, 14, 14)\n",
    "        )\n",
    "        # Block 2: Conv(16→32, 3×3) + BN + ReLU + MaxPool(2)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # (32, 14, 14)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (32, 7, 7)\n",
    "        )\n",
    "        # Block 3: Conv(32→64, 3×3) + BN + ReLU\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # (64, 7, 7)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "cnn_net = ConvNet().to(device)\n",
    "print(cnn_net)\n",
    "\n",
    "# 전체 파라미터 수\n",
    "cnn_params = sum(p.numel() for p in cnn_net.parameters())\n",
    "print(f\"\\nCNN 전체 파라미터 수: {cnn_params:,}개\")\n",
    "\n",
    "# 블록별 파라미터 분석\n",
    "print(f\"\\n{'블록':15s} {'파라미터 수':>12s}\")\n",
    "print(\"-\" * 30)\n",
    "for name, module in [(\"Block 1\", cnn_net.block1),\n",
    "                      (\"Block 2\", cnn_net.block2),\n",
    "                      (\"Block 3\", cnn_net.block3),\n",
    "                      (\"Classifier\", cnn_net.classifier)]:\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"{name:15s} {params:>12,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 shape 확인\n",
    "for images, labels in train_dataloader:\n",
    "    print(f\"배치 이미지 shape: {images.shape}\")\n",
    "    print(f\"배치 레이블 shape: {labels.shape}\")\n",
    "    print(f\"\\n→ CNN은 (N, C, H, W) 형태 그대로 사용 (Flatten 불필요)\")\n",
    "    print(f\"  FC NN: {images.shape} → view(-1, 784) → (64, 784) 로 펼쳐야 함\")\n",
    "    print(f\"  CNN:   {images.shape} → 그대로 입력!\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. CNN 학습\n",
    "\n",
    "- **옵티마이저**: Adam (적응적 학습률, SGD보다 빠른 수렴)\n",
    "- **손실 함수**: CrossEntropyLoss (모델에 LogSoftmax 없이 직접 사용)\n",
    "- **에포크**: 15회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cnn = torch.optim.Adam(cnn_net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epochs = 15\n",
    "cnn_costs = []\n",
    "cnn_times = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_net.train()\n",
    "    start_time = time.time()\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_dataloader)\n",
    "\n",
    "    for X, Y in train_dataloader:\n",
    "        X = X.to(device)  # (N, 1, 28, 28) - Flatten 하지 않음!\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer_cnn.zero_grad()\n",
    "        hypothesis = cnn_net(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer_cnn.step()\n",
    "\n",
    "        avg_cost += cost.item() / total_batch\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    cnn_costs.append(avg_cost)\n",
    "    cnn_times.append(elapsed)\n",
    "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | cost = {avg_cost:.6f} | time = {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\n평균 에포크 시간: {sum(cnn_times)/len(cnn_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. 특징 맵(Feature Map) 시각화\n",
    "\n",
    "CNN의 각 블록이 이미지에서 어떤 특징을 추출하는지 시각적으로 확인합니다.\n",
    "\n",
    "- **Block 1**: 엣지, 윤곽선 등 저수준 특징\n",
    "- **Block 2**: 텍스처, 패턴 등 중간 수준 특징\n",
    "- **Block 3**: 부분 형태, 의미적 특징 등 고수준 특징\n",
    "\n",
    "블록이 깊어질수록 더 추상적인 특징을 학습하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_net.eval()\n",
    "sample_img = test_data[0][0].unsqueeze(0).to(device)\n",
    "sample_label = test_data[0][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    feat1 = cnn_net.block1(sample_img)\n",
    "    feat2 = cnn_net.block2(feat1)\n",
    "    feat3 = cnn_net.block3(feat2)\n",
    "\n",
    "print(f\"원본 이미지: {class_names[sample_label]}\")\n",
    "print(f\"Block 1 출력: {feat1.shape} (16채널, 14×14)\")\n",
    "print(f\"Block 2 출력: {feat2.shape} (32채널, 7×7)\")\n",
    "print(f\"Block 3 출력: {feat3.shape} (64채널, 7×7)\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "fig.suptitle(f\"Feature Maps — 입력: {class_names[sample_label]}\", fontsize=14)\n",
    "\n",
    "# 원본 이미지 (첫 번째 행 왼쪽에 표시)\n",
    "axes[0, 0].imshow(sample_img[0, 0].cpu(), cmap=\"gray\")\n",
    "axes[0, 0].set_title(\"Original\", fontsize=9)\n",
    "axes[0, 0].axis(\"off\")\n",
    "for i in range(1, 8):\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "# Block 1 feature maps (8개)\n",
    "for i in range(8):\n",
    "    axes[1, i].imshow(feat1[0, i].cpu(), cmap=\"viridis\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Block 1\\n(14×14)\", fontsize=10)\n",
    "\n",
    "# Block 2 feature maps (8개)\n",
    "for i in range(8):\n",
    "    axes[2, i].imshow(feat2[0, i].cpu(), cmap=\"viridis\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[2, i].set_ylabel(\"Block 2\\n(7×7)\", fontsize=10)\n",
    "\n",
    "# Block 3 feature maps (8개)\n",
    "for i in range(8):\n",
    "    axes[3, i].imshow(feat3[0, i].cpu(), cmap=\"viridis\")\n",
    "    axes[3, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[3, i].set_ylabel(\"Block 3\\n(7×7)\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. FC NN vs CNN 비교\n",
    "\n",
    "2편에서 학습한 FC NN을 동일한 조건(Adam, 15 에포크)으로 재학습하여 CNN과 공정하게 비교합니다.\n",
    "\n",
    "- **FC 1-Layer**: 784 → 10 (단순 선형 분류)\n",
    "- **FC Multi-Layer**: 784 → 512 → 256 → 10 (3계층 분류기)\n",
    "- **CNN (3-Block)**: Conv 3블록 + Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC 1-Layer 모델\n",
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# FC Multi-Layer 모델\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# --- FC 1-Layer 학습 ---\n",
    "fc1_net = SingleLayerNet().to(device)\n",
    "fc1_opt = torch.optim.Adam(fc1_net.parameters(), lr=0.001)\n",
    "fc1_costs, fc1_times = [], []\n",
    "\n",
    "print(\"FC 1-Layer 학습 중...\")\n",
    "for epoch in range(num_epochs):\n",
    "    fc1_net.train()\n",
    "    start_time = time.time()\n",
    "    avg_cost = 0\n",
    "    for X, Y in train_dataloader:\n",
    "        X, Y = X.view(-1, 784).to(device), Y.to(device)\n",
    "        fc1_opt.zero_grad()\n",
    "        cost = criterion(fc1_net(X), Y)\n",
    "        cost.backward()\n",
    "        fc1_opt.step()\n",
    "        avg_cost += cost.item() / len(train_dataloader)\n",
    "    fc1_costs.append(avg_cost)\n",
    "    fc1_times.append(time.time() - start_time)\n",
    "print(f\"  완료 — 최종 cost: {fc1_costs[-1]:.6f}\")\n",
    "\n",
    "# --- FC Multi-Layer 학습 ---\n",
    "fcm_net = MultiLayerNet().to(device)\n",
    "fcm_opt = torch.optim.Adam(fcm_net.parameters(), lr=0.001)\n",
    "fcm_costs, fcm_times = [], []\n",
    "\n",
    "print(\"FC Multi-Layer 학습 중...\")\n",
    "for epoch in range(num_epochs):\n",
    "    fcm_net.train()\n",
    "    start_time = time.time()\n",
    "    avg_cost = 0\n",
    "    for X, Y in train_dataloader:\n",
    "        X, Y = X.view(-1, 784).to(device), Y.to(device)\n",
    "        fcm_opt.zero_grad()\n",
    "        cost = criterion(fcm_net(X), Y)\n",
    "        cost.backward()\n",
    "        fcm_opt.step()\n",
    "        avg_cost += cost.item() / len(train_dataloader)\n",
    "    fcm_costs.append(avg_cost)\n",
    "    fcm_times.append(time.time() - start_time)\n",
    "print(f\"  완료 — 최종 cost: {fcm_costs[-1]:.6f}\")\n",
    "\n",
    "print(\"\\nFC 1-Layer / FC Multi-Layer / CNN 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 테스트 정확도 평가 ---\n",
    "def evaluate_model(model, dataloader, flatten=False):\n",
    "    \"\"\"모델의 전체 정확도와 클래스별 정확도를 계산합니다.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0] * 10\n",
    "    class_total = [0] * 10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, Y in dataloader:\n",
    "            if flatten:\n",
    "                X = X.view(-1, 784)\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += Y.size(0)\n",
    "            correct += (predicted == Y).sum().item()\n",
    "            for i in range(Y.size(0)):\n",
    "                label = Y[i].item()\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    overall_acc = 100 * correct / total\n",
    "    class_acc = {\n",
    "        class_names[i]: 100 * class_correct[i] / class_total[i]\n",
    "        for i in range(10)\n",
    "    }\n",
    "    return overall_acc, class_acc\n",
    "\n",
    "\n",
    "fc1_acc, fc1_class_acc = evaluate_model(fc1_net, test_dataloader, flatten=True)\n",
    "fcm_acc, fcm_class_acc = evaluate_model(fcm_net, test_dataloader, flatten=True)\n",
    "cnn_acc, cnn_class_acc = evaluate_model(cnn_net, test_dataloader, flatten=False)\n",
    "\n",
    "# 클래스별 정확도 출력 (CNN)\n",
    "print(\"CNN 클래스별 테스트 정확도:\")\n",
    "print(\"-\" * 35)\n",
    "for name, acc in cnn_class_acc.items():\n",
    "    bar = \"#\" * int(acc / 2)\n",
    "    print(f\"  {name:15s} {acc:5.1f}% {bar}\")\n",
    "print(f\"\\n  {'전체 정확도':15s} {cnn_acc:5.1f}%\")\n",
    "\n",
    "# --- 비교 차트 ---\n",
    "fc1_params = sum(p.numel() for p in fc1_net.parameters())\n",
    "fcm_params = sum(p.numel() for p in fcm_net.parameters())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# (1) Cost 비교\n",
    "axes[0].plot(epochs, fc1_costs, \"o-\", label=f\"FC 1-Layer\", markersize=3)\n",
    "axes[0].plot(epochs, fcm_costs, \"s-\", label=f\"FC Multi-Layer\", markersize=3)\n",
    "axes[0].plot(epochs, cnn_costs, \"^-\", label=f\"CNN (3-Block)\", markersize=3)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Cost\")\n",
    "axes[0].set_title(\"Training Cost\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# (2) 에포크별 학습 시간\n",
    "axes[1].plot(epochs, fc1_times, \"o-\", label=\"FC 1-Layer\", markersize=3)\n",
    "axes[1].plot(epochs, fcm_times, \"s-\", label=\"FC Multi-Layer\", markersize=3)\n",
    "axes[1].plot(epochs, cnn_times, \"^-\", label=\"CNN (3-Block)\", markersize=3)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Time (seconds)\")\n",
    "axes[1].set_title(\"Training Time per Epoch\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# (3) 테스트 정확도 비교 (막대 그래프)\n",
    "model_names = [\"FC 1-Layer\", \"FC Multi-Layer\", \"CNN (3-Block)\"]\n",
    "accuracies = [fc1_acc, fcm_acc, cnn_acc]\n",
    "colors = [\"#4ECDC4\", \"#45B7D1\", \"#FF6B6B\"]\n",
    "bars = axes[2].bar(model_names, accuracies, color=colors, edgecolor=\"black\", linewidth=0.5)\n",
    "axes[2].set_ylabel(\"Test Accuracy (%)\")\n",
    "axes[2].set_title(\"Test Accuracy Comparison\")\n",
    "axes[2].set_ylim(70, 100)\n",
    "axes[2].grid(True, alpha=0.3, axis=\"y\")\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    axes[2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.3,\n",
    "                 f\"{acc:.1f}%\", ha=\"center\", va=\"bottom\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 결과 요약 테이블 ---\n",
    "print(f\"\\n{'모델':18s} {'파라미터':>12s} {'평균 시간(s)':>12s} {'최종 Cost':>12s} {'테스트 정확도':>12s}\")\n",
    "print(\"=\" * 72)\n",
    "print(f\"{'FC 1-Layer':18s} {fc1_params:>12,} {sum(fc1_times)/num_epochs:>12.2f} {fc1_costs[-1]:>12.6f} {fc1_acc:>11.1f}%\")\n",
    "print(f\"{'FC Multi-Layer':18s} {fcm_params:>12,} {sum(fcm_times)/num_epochs:>12.2f} {fcm_costs[-1]:>12.6f} {fcm_acc:>11.1f}%\")\n",
    "print(f\"{'CNN (3-Block)':18s} {cnn_params:>12,} {sum(cnn_times)/num_epochs:>12.2f} {cnn_costs[-1]:>12.6f} {cnn_acc:>11.1f}%\")\n",
    "print()\n",
    "print(f\"CNN vs FC Multi-Layer:\")\n",
    "print(f\"  파라미터: {cnn_params:,} vs {fcm_params:,} ({cnn_params/fcm_params*100:.1f}%)\")\n",
    "print(f\"  정확도:   {cnn_acc:.1f}% vs {fcm_acc:.1f}% (차이: {cnn_acc - fcm_acc:+.1f}%p)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## CNN이 효과적인 이유\n",
    "\n",
    "| 특성 | FC (완전연결) | CNN (합성곱) |\n",
    "|---|---|---|\n",
    "| 입력 처리 | 모든 픽셀을 1차원으로 펼침 | 2D 공간 구조를 유지 |\n",
    "| 연결 방식 | 모든 입출력이 연결 | 커널 크기만큼 지역 연결 |\n",
    "| 파라미터 규모 | 입력 × 출력 (이미지 커질수록 폭발) | 커널 크기 × 채널 수 (이미지 크기 무관) |\n",
    "| 특징 학습 | 전역적 패턴만 학습 | 계층적 특징 학습 |\n",
    "\n",
    "CNN이 이미지에 강한 3가지 핵심 이유:\n",
    "\n",
    "1. **지역성(Locality)**: 이미지의 의미 있는 패턴은 인접 픽셀에서 나옵니다. CNN의 작은 커널이 이를 효과적으로 포착합니다.\n",
    "\n",
    "2. **이동 불변성(Translation Invariance)**: 동일한 필터가 이미지 전체를 스캔하므로, 패턴이 어디에 있든 동일하게 감지합니다.\n",
    "\n",
    "3. **계층적 특징(Hierarchical Features)**: 블록이 깊어질수록 추상적인 특징을 학습합니다.\n",
    "   - Block 1: 엣지, 코너, 텍스처\n",
    "   - Block 2: 패턴, 부분 형태\n",
    "   - Block 3: 객체 형태, 의미적 특징\n",
    "\n",
    "---\n",
    "\n",
    "다음 노트북에서는 **GAN(생성적 적대 신경망)**으로 이미지를 '분류'하는 것을 넘어 '생성'하는 방법을 학습합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}