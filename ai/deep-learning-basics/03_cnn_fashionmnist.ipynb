{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3편: CNN으로 이미지 분류하기\n",
    "\n",
    "이 노트북에서는 다음을 학습합니다:\n",
    "- 합성곱(Convolution) 연산의 원리\n",
    "- CNN의 핵심 구성 요소 (Conv2d, ReLU, MaxPool2d, Dropout)\n",
    "- FashionMNIST를 CNN으로 분류\n",
    "- FC NN vs CNN 성능/효율 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 합성곱(Convolution)이란?\n",
    "\n",
    "FC 레이어는 이미지의 모든 픽셀을 1차원으로 펼쳐서 처리합니다.\n",
    "이 과정에서 **인접 픽셀 간의 공간적 관계(지역성)**가 완전히 사라집니다.\n",
    "\n",
    "합성곱은 작은 **필터(커널)**를 이미지 위에서 슬라이딩하며 특징을 추출합니다:\n",
    "- **지역적 연결**: 전체가 아닌 작은 영역(예: 3×3)만 연결\n",
    "- **파라미터 공유**: 동일한 필터를 이미지 전체에 재사용\n",
    "- **결과**: 파라미터 수가 이미지 크기와 무관 → 대폭 감소!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# FashionMNIST 데이터 준비\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=True, download=True, transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\", train=False, download=True, transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "class_names = [\n",
    "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
    "]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"사용 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN 핵심 구성 요소\n",
    "\n",
    "| 레이어 | 역할 |\n",
    "|---|---|\n",
    "| `Conv2d` | 합성곱 연산으로 특징 추출 (커널이 이미지 위를 슬라이딩) |\n",
    "| `ReLU` | 비선형성 추가 (음수 → 0, 양수 → 그대로) |\n",
    "| `MaxPool2d` | 공간 크기 축소 (2×2 영역에서 최대값만 추출) |\n",
    "| `Dropout` | 학습 중 일부 뉴런을 무작위로 비활성화 (과적합 방지) |\n",
    "| `Linear` | 최종 분류를 위한 완전연결 레이어 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2d 동작 시각화\n",
    "sample_img = training_data[0][0].unsqueeze(0)  # (1, 1, 28, 28)\n",
    "print(f\"입력 이미지 shape: {sample_img.shape}\")\n",
    "\n",
    "# 3×3 커널로 합성곱 적용\n",
    "conv_layer = nn.Conv2d(1, 1, kernel_size=3, padding=0)\n",
    "with torch.no_grad():\n",
    "    conv_output = conv_layer(sample_img)\n",
    "print(f\"Conv2d(1→1, 3×3) 출력 shape: {conv_output.shape}\")\n",
    "print(f\"  → 28-3+1 = 26, 크기가 약간 줄어듦\")\n",
    "\n",
    "# MaxPool2d 적용\n",
    "pool_output = F.max_pool2d(conv_output, 2)\n",
    "print(f\"MaxPool2d(2) 출력 shape: {pool_output.shape}\")\n",
    "print(f\"  → 26/2 = 13, 크기가 절반으로 축소\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CNN 모델 구현\n",
    "\n",
    "```\n",
    "입력 (1, 28, 28)\n",
    "  → Conv2d(1→32, 3×3) → ReLU   → (32, 26, 26)\n",
    "  → Conv2d(32→64, 3×3) → ReLU  → (64, 24, 24)\n",
    "  → MaxPool2d(2)                → (64, 12, 12)\n",
    "  → Dropout(0.25)\n",
    "  → Flatten                     → (9216)\n",
    "  → Linear(9216→128) → ReLU\n",
    "  → Dropout(0.5)\n",
    "  → Linear(128→10)\n",
    "  → LogSoftmax → 출력 (10 클래스)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "cnn_net = ConvNet().to(device)\n",
    "print(cnn_net)\n",
    "\n",
    "cnn_params = sum(p.numel() for p in cnn_net.parameters())\n",
    "print(f\"\\nCNN 파라미터 수: {cnn_params:,}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 shape 확인\n",
    "for images, labels in train_dataloader:\n",
    "    print(f\"배치 이미지 shape: {images.shape}\")\n",
    "    print(f\"  → CNN은 (N, C, H, W) 형태 그대로 사용 (Flatten 불필요)\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cnn = torch.optim.SGD(cnn_net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "cnn_costs = []\n",
    "cnn_times = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_dataloader)\n",
    "\n",
    "    for X, Y in train_dataloader:\n",
    "        X = X.to(device)  # (N, 1, 28, 28) - Flatten 하지 않음!\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer_cnn.zero_grad()\n",
    "        hypothesis = cnn_net(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer_cnn.step()\n",
    "\n",
    "        avg_cost += cost.item() / total_batch\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    cnn_costs.append(avg_cost)\n",
    "    cnn_times.append(elapsed)\n",
    "    print(f\"Epoch {epoch+1:02d} | cost = {avg_cost:.6f} | time = {elapsed:.2f}s\")\n",
    "\n",
    "print(f\"\\n평균 에포크 시간: {sum(cnn_times)/len(cnn_times):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FC NN vs CNN 비교\n",
    "\n",
    "2편에서 학습한 FC NN 결과를 재현하여 CNN과 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC NN 재학습 (비교용)\n",
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class MultiLayerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(28 * 28, 28 * 28 * 10)\n",
    "        self.linear2 = nn.Linear(28 * 28 * 10, 28 * 28 * 10)\n",
    "        self.linear3 = nn.Linear(28 * 28 * 10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear3(self.linear2(self.linear1(x)))\n",
    "\n",
    "# 1계층 FC 학습\n",
    "fc1_net = SingleLayerNet().to(device)\n",
    "fc1_opt = torch.optim.SGD(fc1_net.parameters(), lr=0.001, momentum=0.9)\n",
    "fc1_costs, fc1_times = [], []\n",
    "\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    avg_cost = 0\n",
    "    for X, Y in train_dataloader:\n",
    "        X, Y = X.view(-1, 784).to(device), Y.to(device)\n",
    "        fc1_opt.zero_grad()\n",
    "        cost = criterion(fc1_net(X), Y)\n",
    "        cost.backward()\n",
    "        fc1_opt.step()\n",
    "        avg_cost += cost.item() / len(train_dataloader)\n",
    "    fc1_costs.append(avg_cost)\n",
    "    fc1_times.append(time.time() - start_time)\n",
    "\n",
    "# 3계층 FC 학습\n",
    "fc3_net = MultiLayerNet().to(device)\n",
    "fc3_opt = torch.optim.SGD(fc3_net.parameters(), lr=0.001, momentum=0.9)\n",
    "fc3_costs, fc3_times = [], []\n",
    "\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    avg_cost = 0\n",
    "    for X, Y in train_dataloader:\n",
    "        X, Y = X.view(-1, 784).to(device), Y.to(device)\n",
    "        fc3_opt.zero_grad()\n",
    "        cost = criterion(fc3_net(X), Y)\n",
    "        cost.backward()\n",
    "        fc3_opt.step()\n",
    "        avg_cost += cost.item() / len(train_dataloader)\n",
    "    fc3_costs.append(avg_cost)\n",
    "    fc3_times.append(time.time() - start_time)\n",
    "\n",
    "print(\"FC 1-Layer / FC 3-Layer / CNN 학습 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_params = sum(p.numel() for p in fc1_net.parameters())\n",
    "fc3_params = sum(p.numel() for p in fc3_net.parameters())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "epochs = range(1, 11)\n",
    "\n",
    "# Cost 비교\n",
    "axes[0].plot(epochs, fc1_costs, \"o-\", label=f\"FC 1-Layer ({fc1_params:,})\")\n",
    "axes[0].plot(epochs, fc3_costs, \"s-\", label=f\"FC 3-Layer ({fc3_params:,})\")\n",
    "axes[0].plot(epochs, cnn_costs, \"^-\", label=f\"CNN ({cnn_params:,})\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Cost\")\n",
    "axes[0].set_title(\"Cost Comparison\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 시간 비교\n",
    "axes[1].plot(epochs, fc1_times, \"o-\", label=\"FC 1-Layer\")\n",
    "axes[1].plot(epochs, fc3_times, \"s-\", label=\"FC 3-Layer\")\n",
    "axes[1].plot(epochs, cnn_times, \"^-\", label=\"CNN\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Time (seconds)\")\n",
    "axes[1].set_title(\"Training Time per Epoch\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 요약 테이블\n",
    "print(f\"{'모델':15s} {'파라미터':>15s} {'평균 시간(s)':>12s} {'최종 Cost':>12s}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'FC 1-Layer':15s} {fc1_params:>15,} {sum(fc1_times)/10:>12.2f} {fc1_costs[-1]:>12.6f}\")\n",
    "print(f\"{'FC 3-Layer':15s} {fc3_params:>15,} {sum(fc3_times)/10:>12.2f} {fc3_costs[-1]:>12.6f}\")\n",
    "print(f\"{'CNN':15s} {cnn_params:>15,} {sum(cnn_times)/10:>12.2f} {cnn_costs[-1]:>12.6f}\")\n",
    "print()\n",
    "print(f\"CNN은 FC 3-Layer 대비:\")\n",
    "print(f\"  파라미터: {cnn_params/fc3_params*100:.1f}% (약 {fc3_params/cnn_params:.0f}배 적음)\")\n",
    "print(f\"  학습시간: {sum(cnn_times)/sum(fc3_times)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN이 효과적인 이유\n",
    "\n",
    "| FC (완전연결) | CNN (합성곱) |\n",
    "|---|---|\n",
    "| 모든 픽셀을 1차원으로 펼침 | 2D 공간 구조를 유지 |\n",
    "| 모든 입출력이 연결 | 커널 크기만큼만 지역 연결 |\n",
    "| 파라미터 = 입력 × 출력 | 파라미터 = 커널 크기 × 채널 수 |\n",
    "| 이미지 크기에 비례하여 폭발 | 이미지 크기와 무관 |\n",
    "\n",
    "핵심 아이디어: **이미지는 지역적 패턴의 조합**이다.\n",
    "- 저수준 특징: 엣지, 코너, 텍스처\n",
    "- 고수준 특징: 형태, 패턴, 객체\n",
    "- CNN은 이 계층 구조를 자연스럽게 학습\n",
    "\n",
    "→ **다음 노트북에서 GAN(생성적 적대 신경망)으로 이미지를 '생성'하는 방법을 학습합니다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
